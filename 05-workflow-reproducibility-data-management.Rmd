
# Workflow, reproducibility, and data management {#workflow}

```{r setup, include=F}
knitr::opts_chunk$set(echo = F, warning = F, message = F, dpi = 72)
library(magrittr)
```



This section is a brief, non-technical manifesto about what I am learning in graduate school besides soil science.
The majority of this proposal has focused on the specifics of my research project.
Needless to say, this is the most important part of getting my PhD. 

I feel that as a group, scientists do not give enough attention to the _actual mechanics of how work is achieved_.
Practical "workflow skills" are rarely taught at the undergraduate or graduate level.
I guess it is assumed that one's workflow is simply an individual preference, or that everybody just _knows_ how to organize large projects. 
I think these are both mistakes. 

This section outlines the practices I have adopted in my day-to-day work.
They are relevant to this proposal because these habits help ensure the results of my work are correct and reproducible.
I want my discoveries to be useful both to others and to my future self. 
This requires some best practices as the data are collected, stored, and analyzed. 


My routines are rather different from a typical Microsoft Office-based workflow, and they might seem extreme or unnecessary to Office loyalists.
However, I feel strongly about this topic and am proud of how I've leveraged computational tools to improve the quality and efficiency of what I do.

Some "computer-y" people are very condescending toward non-programmers, and that certainly does not reflect my view. 
Plenty of outstanding science has been achieved using basic programs such as the MS Office suite.
Indeed, most of humankind's greatest discoveries were made before we had computers at all. 

So, although this section could be seen as preachy, it is not intended in such a way. 
I mostly wish to share my gratitude for those who've built tools for reproducible research and to demonstrate how they have helped me.
I hope to teach some of these habits to my own future students. 


<!-- I want to be clear I am _not_ asserting that this is the only "right" way to do things.... -->
<!-- after all, humankind's greatest scientific discoveries were made before there were computers at all, and a lot of excellent science has been done using simple tools and software.  -->
<!-- I feel lucky to have learned some alternatives, and I describe them here.  -->


#### File formats 

I store all my tabular raw data as `.csv` files and I considered them read-only. 
A `.csv` file contains no formatting and is an open format, so it can be read by any operating system without proprietary software.
Compare this to the widespread `.xlsx` format which (while technically an "open") format can also store formulas and other formatting; these can easily be corrupted when transferring data across computers or software versions. 

To ensure the data is kept in its original state, I never perform any direct operations on raw data files. 
If I discover an error, I make a new copy and correct it with an R script so there is a record of what was done. 
I store my data on a local hard drive which is automatically backed up with PSU's subscription to Microsoft OneDrive for Business. 
I use a number of other principles for organizing my tabular data.
I learned most of these from an entertaining and useful paper, which I highly recommend:  [Data Organization in Spreadsheets](https://doi.org/10.1080/00031305.2017.1375989) [@Broman2018].
Hadley Wickham's "tidy" data concept [@Wickham2014] also factors heavily in my data collection and storage routines. 

#### File naming conventions 

I am very cautious about how I name files.
At first blush this sounds like a trivial topic, but I think it is extremely important and highly overlooked.
When returning to a project after a hiatus, well-named files mean we can pick up right where we left off.

I never use spaces in file names because these are problematic when sorting and parsing the characters. 
I use dashes to separate words and underscores to separate pieces of information. 
I always write dates using the ISO standard notation: `YYYY-MM-DD`.
This makes sorting dates much easier because they are naturally ordered from oldest to newest.
Compare this with the U.S. convention of `DD-MM-YYYY`; these will be sorted by the day of the month and not in true temporal order. 
Finally, I design file names so they convey all relevant information about the file's contents regardless of the sub-folder in which it resides. 
Here is an example file name for one of my hundreds of `.ply` meshes:


`cleatmarkmethod_cyl04_2021-04-11.ply`


Because of its consistent separators, this file name can be easily parsed into three pieces of information: the name of the experiment, the cylinder ID, and the date.
Note the leading zeroes before single digits - since we have 12 cylinders, this prevents the computer from sorting the files as:

<!-- \noindent `cyl1` \newline  -->
<!-- `cyl10` \newline  -->
<!-- `cyl11` \newline  -->
<!-- `cyl12` \newline -->
<!-- `cyl2` \newline -->
<!-- `cyl3` \newline -->
<!-- `cyl4` \newline -->
<!-- ... -->

````
cyl1
cyl10
cyl11
cyl1
cyl
cyl
cyl
...
````

The cylinder ID can be easily joined with a metadata file containing more information about the specimens, but these three pieces of information are all that is needed to uniquely identify the file. 

#### Day-to-day lab notebook {#lab-notebook}

I have iterated through almost every possible means of keeping a "lab notebook": paper notebooks, Microsoft Excel, Microsoft Word, Google docs, Evernote, and Microsoft OneNote.
Each is better than nothing, but in my view all have significant flaws. 

I have finally settled on plain text written in either Pandoc's extended Markdown or with R Markdown, for reasons described in the section below on [finished output](#r-markdown-finished-output).
This is a robust system for recording my thoughts and also including images, web links, and photos. 
The other tools described below help to track the status and provenance of a project, but there is no substitute for simply writing down what I did. 


#### Data wrangling and analysis {#data-wrangling-and-analysis}

I perform most data analyses using R, with occasional use of Python.
I mostly use a group of packages known as the **tidyverse** (<https://www.tidyverse.org/>) [@R-tidyverse].
The **tidyverse** makes many common and frustrating data wrangling routines much, much easier.
This frees up time and cognitive load for higher-order tasks. 
Tidyverse code is easy to write and it makes for a simple transition from Excel or SAS.

Many believe that R is just a free statistics program, but this is a misconception.
R is a fully-functioning programming language, so it can do much more than statistics. 
However, R _is_ distinct from most other programming languages in that it was designed for flexibility and interactive data analysis.
This lends it a fluency which is less present in more general-purpose languages such as C++ or Python.

R is open-source and can perform all the routine statistical procedures available in programs such as SAS or SPSS. 
However, unlike its proprietary alternatives, R can be extended by anybody who wishes to do so.
This means that the menu of available analyses is limited only by the user's imagination and computational skill. 

#### Figures and tables {#figures-and-tables}

I produce figures using the **ggplot2** package in R [@R-ggplot2]. 
This package is built around the principles outlined in \underline{The Grammar of Graphics} [@Wilkinson2005].
Wilkinson's book describes a theoretical underpinning for building data graphics, and **ggplot2** provides an implementation of the theory which is equally suitable for beginners and advanced users. 

 
I usually produce tables with **kableExtra** in R [@R-kableExtra].
This package allows fine-grained control over formatting while maintaining the link between raw data, source code, and finished output. 
Candidly, I often resent this package's documentation and user interface, but I still use it because of its powerful formatting capabilities and because it can output tables to a variety of document formats^[I much prefer figures to tables - a good visualization shows the data in a way a table never could.
Obviously, not all information is amenable to graphical presentation.]. 


#### Finished output {#r-markdown-finished-output}

I generate reports, presentations, and papers with the **knitr** and **rmarkdown** R packages (<https://rmarkdown.rstudio.com/>) [@R-knitr; @R-rmarkdown].
R Markdown effectively supplants MS Office programs: it directly weaves one's writing with the source code for a data analysis.
This means that there is no need to copy-paste results into a "finished" document. 
By eliminating copy-paste, R Markdown makes the analysis process not just easier but **more likely to be correct**.
For a quick and amusing take on the value of R Markdown , I recommend watching [this 2-minute video](https://www.youtube.com/watch?v=s3JldKoA0zw).

An R Markdown file is a written as plain text with no formatting - this is rather different than programs such as Microsoft Word which display formatted text as you type it. 
Markdown instead uses special characters such as `*`, `~` and `#` to provide semantic instructions about the document's structure and formatting. 
One of the best features of R Markdown is that it can be used to generate many, many types of output. 
These include pdf files, HTML documents, websites, slide shows, posters, and others. 

In one step, R Markdown runs your analysis code, weaves the output into a Markdown file, and then calls a command-line tool called Pandoc (<https://pandoc.org/>) to translate the Markdown document into more complex markup languages such as \LaTeX, HTML, or XML. Finally, Pandoc converts the document to a finished output. 
The simplicity of Pandoc's Markdown abstracts away the complex sytnax of these other languages. This means the writer can focus on the content instead of minutae like whether an angle bracket or backslash was forgotten. 
Happily, Pandoc also supports the inclusion of raw \LaTeX{} or HTML sytnax. 
This means all the advanced features of these languages are available when Markdown alone will not suffice.

#### Package writing and testing  {#package-writing-and-testing}

Since I repeatedly perform many of the same analyses with different data, it is wise to make them re-usable. 
Instead of copy-pasting a script and then changing one or two things in it, I write re-usable functions and store them in my own R packages.
This approach reflects the [Unix philosophy](https://en.wikipedia.org/wiki/Unix_philosophy) of writing small programs which do only one thing well, but can easily communicate with one another.
It also simplifies debugging - to find a needle in a haystack, it helps to keep the haystacks small. 

Writing packages also lets me write documentation which lives right beside the functions.
If I forget how a function works, I just press `F1` and get immediate help in RStudio.
Finally, writing packages makes the code very easy to share with others. Instead of providing very detailed instructions about how to use the script -- with the possibility of breaking file paths, etc. -- the other person can simply install the package and use the functions however they please. 
Unlike a spreadsheet, there is no way to "break" R functions by changing a cell reference or re-arranging the data. 
I haven't promoted any of my packages yet, but I plan to publish my **soiltestr** package on CRAN (the major R package clearinghouse) and write a paper on it for the SSSAJ.

One challenge in package development is that by adding new functions or making changes, it _is_ possible to break existing code. 
To prevent this from happening I write re-usable tests for any new functions using the **testthat** package in R [@R-testthat]. 
Before making the changes permanent, I can simply call `devtools::test()` to re-run all the tests - this ensure the existing functions are still behaving the way they are supposed to. 
For example, I recently added a function to handle pre-treatment corrections for particle size analysis. 
I have a test which checks whether a particular sample data file returns a clay content of 26.2%. 
If the function returns something different, I immediately know that something has broken and I can fix it right away.

#### Pipeline automation  {#pipeline-automation}

R Markdown is an excellent tool for generating outputs, but it is rare that an entire project can be jammed into a single `.Rmd` document.
For example, a research project might generate a presentation for students, a report to funders, and a final manuscript. 
It's likely each output will use some of the same results - figures, tables, model outputs, etc. That means that if we make any changes - for example, by adding new data or by changing a regression model - the figure or table needs to be updated in multiple places. 
That means more work and more chance for mistakes.


To address this issue I now use `Make` to manage my projects.
`Make` is a free and open-source tool maintained by the [GNU project](https://www.gnu.org/software/).
`Make` has no graphical user interface, so its use requires at least basic familiarity with the command line. 

`Make` behaves like a "helicopter parent" - it watches its "children" and it knows if anything about them changes. 
The core concept underlying `Make` is simple: it keeps the project up-to-date by building **target** files from **prerequisites** using a **recipe**. 
Here is an example recipe to build a figure which depends on a data file and an R script:

```makefile
fig-1.pdf:  fig-1.R  clean-data.csv
    Rscript --vanilla fig-1.R
```

This rule states that the otuput (`fig-1.pdf`) should be built using `fig-1.R` and `clean-data.csv` by running the R script `fig-1.R`.
This script will read `clean-data.csv`, generate the plot, and save it as `fig-1.pdf`.^[I prefer to use descriptive names for saving figures rather than numbers. Their order may change later, and a number provides no information about the file's contents. However, a number makes more sense to demonstrate the point in this minimal example.]

The user of GNU `Make` writes plain-text instructions called a `Makefile` which contain a rule for each target. 
There are a number of syntax shortcuts to reduce typing, but all rules still follow the basic structure above.

`Make` now knows how to build the target files and it watches for changes in the prerequisites.
If a prerequisite file is updated, `Make` automatically re-builds any outputs which depend on that prerequisite.
Importantly, `Make` will not do anything for rules which are already up-to-date. This eliminates the need to re-run time-consuming code which has not changed.


Figure \@ref(fig:example-make-dependency-graph) shows a minimal example of a project having two figures and three total "finished products": a paper, a report, and a presentation.
The graph is read from top down. It shows the relationships between inputs (green ovals) and outputs (red ovals).
This graph might seem really complicated, but I would argue it is a lot less complicated than having to _remember_ how all the components of a project fit together. 
I much prefer to define the process explicitly and let the computer handle the "remembering."
This is a simple example, and real projects have a lot more dependencies. 

```{r example-make-dependency-graph, out.width='90%', fig.align='center', fig.cap='Dependency graph of a minimal example Makefile.'}
knitr::include_graphics(here::here(
  'example-mf-project/example-dependency-graph.pdf')
)
```


\newpage

Below is the corresponding `Makefile` for the example project drawn in Figure \@ref(fig:example-make-dependency-graph).

```{r example-makefile-text, echo=FALSE, collapse=TRUE, results='asis'}
show_output <- function(x){
  cat("```makefile", xfun::read_utf8(x), "```", sep = "\n")
}

show_output(here::here('example-mf-project/mfile-w-extensions.makefile'))
```


`Make` is language agnostic, so it can run any programs compatible with a Unix shell.
`Make` syntax is not intuitive and debugging a `Makefile` can be especially challenging.
It was first written in the 1970s by developers in need of an automated means to compile their C programs.
This task is well outside the scope of my own research, but I have still found `Make` invaluable for managing my projects.


#### Version control with Git {#version-control}

We all fear losing our hard work during a computer malfunction. 
It can be useful to store old versions of a project for backup and for later reference.
We might employ a manual version control system by naming files successively, like `report-4.docx.` 
We might also use features such as MS Word's "Track Changes" to record progressive edits. 
Hopefully we avoid a situation like the comic below:

```{r phd-files-comic, echo=FALSE, out.width='50%', message=FALSE, warning=FALSE, fig.align='center'}

gif_convert <- function(url, file){
  
  magick_obj <- magick::image_read(path = url)
  
  converted_magick_obj <- magick::image_convert(magick_obj, format = 'png')
  
  magick::image_write(image = converted_magick_obj, path = file)
  
}

# make tibble of arguments 

images <- tibble::tibble(
  url = c('http://phdcomics.com/comics/archive/phd101212s.gif',
          'http://phdcomics.com/comics/archive/phd052810s.gif'),
  file = as.character(fs::path(
    'images',
    paste0(
      c('final-comic', 
        'file-explorer'),
    '.png')
  )
)
)

# save if they don't yet exist 
purrr::pwalk(images[!file.exists(images$file), ], gif_convert)

# just include the one with all the edits, the other one requires 
# too much contemplation and I'm just going for a quick laugh 
knitr::include_graphics('images/final-comic.png')
```

 
Manual version control is useful but it depends highly on the user's vigilance and it is prone to human error.


Instead, I prefer to use Git for version control. 
You could think of Git as "track changes on steroids."
Git automatically tracks every file in a project, without cumbersome markup and with minimal need for human intervention. 

Version control is even more important as the scope of a project grows. 
If I change something in my re-usable functions, I could risk breaking other existing results.
With Git I never need to worry about this because it is easy to revert to a prior version.
Git is like rock-climbing with safety ropes - it's a bit more work, but it ensures you can only fall so far^[analogy credit: Hadley Wickham, <https://r-pkgs.org/git.html#git-commit>]. 


With the possible exception of GNU `Make`, Git has the steepest learning curve of any of these tools. 
Its syntax is frustrating and many `git` commands have hidden options that are not well-documented. 
When I first started using Git, I actually _lost_ more work by using it than I saved.
However, now that I am a competent user, I would never work without Git.
I access older states of one or more files on an almost weekly basis. 

Git also allows multiple collaborators to keep separate local copies of a project on their own computers. 
A service like GitHub or BitBucket can easily merge the separate versions together. 
This eliminates the need to e-mail files back and forth or use cloud-based systems such as Google Docs. 

Currently I am using Git only to "collaborate" with myself. 
However I anticipate using it in the future to collaborate with my own graduate students and with other scientists.



#### Summary: workflow, reproducibility, data storage {.unnumbered #summary-workflow}

This workflow may seem rather extreme to someone accustomed to using MS Office, Minitab and SAS. 
Indeed, when I first heard of ideas like using code to draw diagrams or ditching Word and PowerPoint, I was initially skeptical.

About a year into my PhD I realized that my existing systems were simply not adequate. 
I began to lose track of some experiments. 
I became increasingly incensed at the notion of wasting my valuable time collecting data that might eventually become useless because of a mix-up. 
I decided something had to change. 

At first I was only interested in R because I saw my friends making pretty graphs with **ggplot2**, and I wanted to do that too - but I quickly became hooked on the idea of reproducible research and all its associated tools.

I do not consider myself to be particularly organized, and I used to think that programming was not for me. 
I have probably over-compensated for these innate tendencies and gone a bit further than necessary. 
However, I feel it's been _more_ than worth the effort and I swear by these routines because they enhance my ability to do high-quality science. 
If I succeed in becoming a professor I would feel an urge (a duty, even) to teach a course on these topics. 
They have helped me so much, and surely I'm not unique...many others could benefit from then too. 


Making research reproducible is hard.
During frustrating moments while debugging \LaTeX or fighting with `Make`, I sometimes wonder if it is worth all the extra effort.
In these moments I remember a quote often uttered by one of my baseball coaches:

>"If you don't have time to do it right, when will you have time to do it over?"






