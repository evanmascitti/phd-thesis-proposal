
# Workflow, reproducibility, and data management {#workflow}

This section is a brief, non-technical manifesto about what I am learning in graduate school - besides soil science.
The majority of this proposal has focused on the specifics of the research project I have undertaken.
Needless to say this is the most important part of getting my PhD. 
However, I have come to feel that in the field of science too little attention is paid to the _actual mechanics of how work is achieved_.
"Workflow skills" are not formally taught at the undergraduate or graduate level.
I guess most of us assume that one's workflow is simply an individual preference, or that everybody just _knows_ how to manage large projects. 
Both of these are mistaken ideas. 

The purpose of this short section is not to pontificate but to transparently describe the practices I have adopted in my day-to-day work.
They are relevant to this proposal because these practices ensure my data is stored safely, and that the results of my work are correct and reproducible.
I want my work to be useful to others and to my future self, and this requires some best practices as the data are collected, stored, and analyzed. 

My routines are rather different than a typical Microsoft Office-based workflow, and they may seem extreme or unnecessary to Office loyalists.
However I feel strongly about this topic and am proud of how I've leveraged computational tools to improve the quality and efficiency of my work.
I hope to eventually teach some of the habits explained below to my own students. 

## Data storage 

I store all my tabular raw data as `.csv` files and I considered these read-only. 
A `.csv` file contains no formatting and is an open format, so it can be read by any operating system without proprietary software.
Compare this to the commonly used `.xlsx` format which (while technically an "open") format can also store formulas and other formatting; these can easily be corrupted when transferring data across computers or software versions. 

To ensure the data is kept in its original state, I never perform any direct operations on raw data files. 
If I discover an error, I make a new copy and correct it with an R script and so there is a record of what was done. 
I store my data on a local hard drive which is automatically backed up with PSU's subscription to Microsoft OneDrive for Business. 

I am very cautious about how I name files.
While this seems like a trivial topic, I actually think it is actually one of the most important and overlooked tasks in research.
I always write dates using the ISO standard notation: YYYY-MM-DD.
This makes sorting dates much easier because they are naturally ordered from oldest to newest.
Compare this with the U.S. convention of DD-MM-YYYY; these are not sorted in true temporal order. 
I never use spaces in file names because these cause problems when sorting and parsing the characters. 
I use dashes to separate words and underscores to separate pieces of information. 
Finally, I design file names so they convey all relevant information about the file's contents regardless of the sub-folder in which it resides. 
Here is an example file name for one of my `.ply` meshes:
  
\begin{center}
`cleatmarkmethod_cyl04_2021-04-11.ply`
\end{center}

Because of its consistent separators, this file name can be easily parsed into three pieces of information: the name of the experiment, the cylinder ID, and the date.
Note the leading zeroes before single digits - since we have 12 cylinders, this prevents the computer from sorting the files into something like:

  `cyl1` \\
`cyl10` \\
`cyl11` \\
`cyl12` \\
`cyl2` \\
...

The cylinder ID can be easily joined with a metadata file containing more information about the specimen, but these three pieces of information are all that is needed to uniquely identify the file. 

## Day-to-day note taking 

I have gone through nearly every possible means of keeping a "lab notebook": paper notebooks, Microsoft Excel, Microsoft Word, Google docs, Evernote, and Microsoft OneDrive.
Each is better than nothing, but in my view all have significant flaws. 

I have finally settled on plain text in either Pandoc's regular Markdown and R Markdown, for reasons described below in Section \@ref(r-markdown-finished-output). 
This is a robust system for recording my thoughts and also including images, web links, and photos. 
The other tools described below help to track the status and provenance of a project, but there is no substitute for simply writing down what I did. 


## Data wrangling and analysis

I perform most analyses using R, with occasional use of Python.
I use a combination of base R and a group of packages known as the **tidyverse** (<https://www.tidyverse.org/>)
The **tidyverse** makes many common and frustrating data wrangling routines much, much easier.
This frees up time and cognitive load for higher-order tasks. 
Tidyverse code is easy to write and it makes for a simple transition from Excel or SAS.

Many believe that R is just for statistics, but this is a misconception.
R is a fully-functioning programming language in which both simple and complex tasks can be performed. 
However, R _is_ distinct from most other programming languages in that it was designed for flexibility and interactive data analysis.
This lends it a fluency which is less present in more general-purpose languages such as C++ or Python.
R is open-source and can perform all the routine statistical procedures available in programs such as SAS or SPSS. 
However, unlike its proprietary alternatives, R can be extended by anybody who wishes to do so.
This means that the menu of available analyses is limited only by the user's imagination and computational skill. 

## Figures and tables 

I produce all my figures using the **ggplot2** package in R. 
This package is built around the principles outlined in \underline{The Grammar of Graphics} [@Wilkinson2005].
Wilkinson's book describes a theoretical underpinning for building data graphics, and **ggplot2** provides an implementation of the theory which is equally suitable for beginners and advanced users. 

I usually produce tables with **kableExtra** in R.
This package allows fine-grained control over formatting while maintaining the link between raw data, source code, and finished output. 
Candidly, I sometimes resent this package's documentation and user interface, but I still use it because of its powerful formatting capabilities and because it can output tables to a variety of document formats. 


## Finished output {#r-markdown-finished-output}

I generate reports, presentations, and papers with the **rmarkdown** package (<https://rmarkdown.rstudio.com/>).
R Markdown effectively supplants MS Office programs: it directly links one's writing with the source code for a data analysis.
This means that there is no need to copy-paste results into a "finished" document. 
By eliminating copy-paste, R Markdown makes the analysis process not just easier but **more likely to be correct**.
For a quick and entertaining take on the value of R Markdown , I recommend watching [this 2-minute video](https://www.youtube.com/watch?v=s3JldKoA0zw).

An R Markdown file is a written as plain text with no formatting - this is rather different than programs such as Microsoft Word which format text as you type it. 
Markdown instead uses special characters such as `*`, `_` and `#` to provide semantic instructions about the document's structure and formatting. 
One of the best features of R Markdown is that it can be used to generate many, many types of output. 
These include pdf files, HTML documents, websites, slide shows, posters, and others. 
Essentially R Markdown (with the help of another command-line tool called Pandoc) translates the formatting instructions you have provided as Markdown into more complex markup languages such as \LaTeX, HTML, or XML.
R Markdown also supports the inclusion of raw \LaTeX{} or HTML sytnax. 

## Package testing 

Since I do many particular analyses repeatedly, it is a good idea to make the functions re-usable. 
Instead of copy-pasting a script and then changing one or two things in it, I write my own R packages. 
This also makes the code very easy to share with others - instead of providing very detailed instructions about how to use the script -- with the possibility of breaking file paths, etc. -- the other person can simply call (for example) `devtools::install_github(evanmascitti/soiltestr)`.

One challenge in package development is that by adding new functions or making changes, it is possible to break existing code. 
To prevent this from happening I write re-usable tests for any new functions using the **testthat** package in R [@R-testthat]. 
Before making the changes permanent, I can simply call `devtools::test()` to re-run all the tests - this ensure the existing functions are still behaving the way they are supposed to. 
For example, I recently added a function to handle pre-treatment corrections for particle size analysis. 
I have a test that checks a particular sample data file returns a clay content of 26.2%. 
If the function returns something different, I immediately know that something has broken and I can fix it right away before the issue is buried or forgotten. 

## Pipeline automation 

R Markdown is an excellent tool, but it is rare that an entire project can be jammed into a single `.Rmd` document.
Most often, I want to re-use or re-combine pieces of an analysis elsewhere. 
A single project might have many finished products, each of which uses some (but not all) of the same components.

To address this issue I now use `Make` to manage my workflow.
`Make` is a command-line tool maintained by the [GNU project](https://www.gnu.org/software/).
There is no graphical user interface, so using `Make` requires at least a basic competence with the Unix shell. 
Like all other GNU programs, `Make` is free and open-source. 
`Make` was first written in the 1970s by developers in need of an automated means to compile their C programs.
Obviously this is well outside the scope of my own research. 
However `Make` is also an excellent tool for automating data analysis pipelines. 

The core concept behind `Make` is simple: it builds _target_ files from _prerequisites_ using a _recipe_. 
Once the user has written a plain-text file known as a `Makefile`, `Make` watches for changes in the prerequisite files.
It then builds a dependency graph (Figure \@ref(fig:example-make-dependency-graph)) and ensures that all target files are kept up to date.
If a dependency file changes, `Make` automatically re-builds any outputs which use that file - but it will not touch other files that are already up-to-date.
This might seem really complicated, but I would argue it is a lot less complicated than having to _remember_ how all the components of a project fit together. I much prefer to define the process explicitly and let the computer handle the "remembering."

```{r example-make-dependency-graph, out.width='90%', fig.align='center'}
knitr::include_graphics('./example-mf-project/dependency-graph.png')
```


As an example, this proposal includes many figures, each which depend on raw data and an R script. 
The very same figures may be used in other places - say, a presentation for students, a presentation for my PhD committee, and a manuscript. 
If I decide to change the title of one of the figures, or add more data, a traditional Office workflow would require me to re-build the figure and then copy it into each of the 4 places it is used. 
With `Make`, all I do is make the change in a single place and then type `make` at the command line. 
All the relevant documents are updated, but without re-running any superfluous code.

The syntax of `Make` is not intuitive and debugging a `Makefile` can be especially challenging because of this tool's internal design.
However I now consider it an invaluable part of managing any project with multiple outputs.

## Version control

We all fear losing our hard work during a computer malfunction. 
It can also be useful to store older versions of a project for later reference.
We might employ a manual version control system, such as naming files like 'report-version-4.docx`, hopefully avoiding a situation like the XKCD comic below:
```{r xkcd-files-comic, out.width='30%', echo=FALSE, message=FALSE, warning=FALSE, fig.align='center'}


# save file
if(!file.exists('images/xkcd-files.png')){
curl::curl_download(url = 'https://imgs.xkcd.com/comics/documents.png',
                    destfile = 'images/xkcd-files.png')
do('https://imgs.xkcd.com/comics/documents.png')
}

knitr::include_graphics('images/xkcd-files.png')
```
This method is useful but highly dependent on the user's vigilance and it is prone to human error.
We might also use features such as MS Word's "Track Changes." 
You could think of Git as "track changes on steroids." Git automatically tracks every file in a project, without cumbersome markup and with minimal need for human intervention. 

Version control is even more important as the scope of a project increases. 
If I change something in my code, I could risk breaking other existing results.
With Git I never need to worry about this because it is easy to revert to a prior version.


With the possible exception of GNU `Make`, Git has the steepest learning curve of any of these tools. 
Its syntax is frustrating and many `git` commands have hidden options that are not well-documented. 
When I first started using Git, I actually _lost_ more work than I saved.
However, now that I am a competent user, I would never work without Git.
I recover older states of one or more files on an almost weekly basis. 

Git also allows multiple collaborators to keep separate local copies of a project on their own computers. 
By using a service like GitHub or BitBucket, the separate versions can be easily merged together. 
This eliminates the need to e-mail files back and forth or use cloud-based systems such as Google Docs (which are useful but not compatible with systems such as R Markdown or `Make`). 

Currently I am using Git only to "collaborate" with myself. 
However I anticipate using it in the future to collaborate with my own future students and with other scientists.



## Summary: workflow, reproducibility, data storage

This workflow may seem rather extreme to someone accustomed to using programs such as Excel, Word, PowerPoint, and SAS. 
When I first heard of seeminlgy crazy ideas like generating tables with code and ditching MS Word, I was initially skeptical.

About a year into my PhD I realized that my existing systems were simply not adequate. 
I began to lose track of some experiments and became increasingly incensed at the notion of wasting my valuable time collecting data that might eventually vaporize. 
I am not a naturally organized person, and I used to think that programming was not for me. 
I have perhaps over-compensated for these innate tendencies but it has been worth the effort.
After the last 18 months I now swear by these routines and I feel grateful to have learned them. 
I know they will further my ability to do high-quality science, and should I eventually secure a faculty position I would feel an urge (a duty, even) to create a graduate level course on these topics. 
I believe they will be tremendously useful to other graduate students. 